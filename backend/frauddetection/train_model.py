import pandas as pd
import numpy as np
import joblib
import lightgbm as lgb
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

# --- 1. SETUP & CONFIGURATION ---
csv_path = 'creditcard.csv' 
if not os.path.exists(csv_path):
    print("FATAL: creditcard.csv not found. Please ensure it's in the current folder.")
    exit()

# CORRECTED COLUMN NAMES (MATCHING YOUR INPUT LIST)
CUSTOMER_ID_COL = 'Customer ID' 
MERCHANT_ID_COL = 'Merchant Name' 
AMOUNT_COL = 'Transaction Amount' 
TIME_COL = 'trans_date_trans_time' 

print("Loading and preprocessing data...")
df = pd.read_csv(csv_path) 

# Convert the combined date/time column to a proper datetime object
try:
    df['TX_DATETIME'] = pd.to_datetime(df[TIME_COL], dayfirst=True) 
except Exception as e:
    print(f"FATAL: Failed to convert column '{TIME_COL}' to datetime. Check format. Error: {e}")
    exit()

# IMPORTANT FIX: Sort by time and reset index
df = df.sort_values('TX_DATETIME').reset_index(drop=True) 

# 2. PREPARE DATA & FEATURE ENGINEERING
target_col = 'is_fraud' 
y_full = df[target_col]

# --- CRITICAL FIX: Isolate columns for reliable rolling window calculation ---
df_work = df[[CUSTOMER_ID_COL, MERCHANT_ID_COL, AMOUNT_COL, 'TX_DATETIME']].copy()

# ====================================================================
# A D V A N C E D   F E A T U R E   E N G I N E E R I N G
# ====================================================================

print("Creating advanced behavioral features...")

# Define the features to calculate
features_to_calc = [
    ('C_T_Freq_7D', CUSTOMER_ID_COL, '7D', AMOUNT_COL, 'count'),
    ('C_Avg_Amt_7D', CUSTOMER_ID_COL, '7D', AMOUNT_COL, 'mean'),
    ('M_T_Freq_30D', MERCHANT_ID_COL, '30D', AMOUNT_COL, 'count')
]
engineered_features_list = []

# Loop through and calculate each rolling feature using a merge-back strategy
for new_col, group_col, window, agg_col, agg_func in features_to_calc:
    # 1. Perform the rolling calculation (Result is a MultiIndex Series)
    rolling_series = df_work.groupby(group_col).rolling(window, on='TX_DATETIME')[agg_col].agg(agg_func)
    
    # 2. Convert the MultiIndex result into a DataFrame with columns matching the MultiIndex levels
    rolling_result_df = rolling_series.reset_index()
    
    # 3. Rename the aggregated value column
    rolling_result_df = rolling_result_df.rename(columns={agg_col: new_col})
    
    # 4. Merge back to the original df_work using the unique keys (ID + TX_DATETIME)
    df_work = df_work.merge(rolling_result_df, on=[group_col, 'TX_DATETIME'], how='left')
    engineered_features_list.append(new_col)

# Extract the new features
engineered_features = df_work[engineered_features_list]

# Start X_full from all numerical features of the original df
X_full = df.select_dtypes(include=[np.number]).copy()
X_full = X_full.set_index(df.index) 

# Merge the new engineered features using the index
X_full = pd.concat([X_full, engineered_features], axis=1)

# Drop redundant/unnecessary numerical columns before scaling
X_full = X_full.drop(columns=[target_col, 'Amount', 'Time'], errors='ignore') 
X_full = X_full.select_dtypes(include=[np.number]) 

# ====================================================================
# F E A T U R E   T R A N S F O R M A T I O N
# ====================================================================

print("Applying feature transformations (Log & Scaling)...")

# 1. Log Transform Skewed Amount Feature
X_full[AMOUNT_COL + '_log'] = np.log(X_full[AMOUNT_COL] + 0.001) 
X_full = X_full.drop(columns=[AMOUNT_COL], errors='ignore') 

# 2. Standard Scale ALL Features
scaler = StandardScaler()
X_full_cols = X_full.columns
# CRITICAL FIX: Preserve the index after scaling
X_full = pd.DataFrame(scaler.fit_transform(X_full), columns=X_full_cols, index=X_full.index)

# Final cleanup (replace NaNs generated by rolling/scaling)
X_full = X_full.fillna(0) 

# --- CRITICAL FINAL ALIGNMENT STEP ---
# This ensures X_full and y_full have the same indices, solving the ValueError
common_index = X_full.index.intersection(y_full.index)
X_full = X_full.loc[common_index]
y_full = y_full.loc[common_index] 

total_rows = len(X_full)
print(f"Total rows used for training: {total_rows}")

# 3. TRAIN & TEST MODEL 
X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)

# --- CALCULATE CLASS WEIGHTS ---
positive_class_count = y_train.sum()
negative_class_count = len(y_train) - positive_class_count
scale_weight_raw = negative_class_count / positive_class_count

# CRITICAL STRATEGY TO DECREASE FALSE ALARMS (FP):
# 1. Lower the weight further to reduce the training bias toward predicting Fraud.
TARGET_LOW_FP_WEIGHT = 5 # <--- ADJUSTED FROM 10 to 5
scale_weight_adjusted = min(scale_weight_raw, TARGET_LOW_FP_WEIGHT) 

print(f"Using AGGRESSIVELY ADJUSTED weight ({scale_weight_adjusted:.2f}) for much lower False Alarm bias.")

# Re-initialize the model with the adjusted class weights
model = lgb.LGBMClassifier(
    random_state=42,
    scale_pos_weight=scale_weight_adjusted, 
    n_estimators=100,
    learning_rate=0.05
)

model.fit(X_train, y_train)
print("Model training complete with aggressively adjusted weight for lower FP.")

# --- 4. OPTIMIZE THRESHOLD & GENERATE STATS ON THE FULL DATASET ---
# Get probabilities
y_proba_full = model.predict_proba(X_full)[:, 1]

# --- Find the Optimal Threshold for the Business ---
# Define Business Costs 
# 2. Increase C_FP to force the optimization routine to find a higher threshold.
C_FN = 500  # Cost of Missed Fraud (Assume $500 average loss)
C_FP = 20   # <--- INCREASED FROM 5 to 20 (Assuming $20 operational cost for a false alarm)

best_threshold = 0.5
min_total_cost = float('inf')

# Systematically test thresholds from 0.01 to 0.99
for threshold in np.linspace(0.01, 0.99, 100):
    y_pred_temp = (y_proba_full >= threshold).astype(int)
    
    # Calculate confusion matrix for the current threshold
    cm = confusion_matrix(y_full, y_pred_temp)
    TN, FP, FN, TP = cm.ravel()
    
    # Calculate the Total Business Cost
    total_cost = (FP * C_FP) + (FN * C_FN)
    
    if total_cost < min_total_cost:
        min_total_cost = total_cost
        best_threshold = threshold

print(f"\nOptimal Threshold (Minimum Business Cost with C_FP={C_FP}): {best_threshold:.4f}")

# Use the best_threshold for final predictions
y_pred_full = (y_proba_full >= best_threshold).astype(int)

# Calculate final confusion matrix and stats
cm_full = confusion_matrix(y_full, y_pred_full)
TN, FP, FN, TP = cm_full.ravel()

# Calculate statistics needed for your dashboard charts
stats_data = {
    "actual_fraud": int(y_full.sum()),
    "actual_legit": int(total_rows - y_full.sum()),
    "pred_fraud": int(TP + FP),       
    "pred_legit": int(TN + FN),       
    "correct_count": int(TN + TP),   
    "false_alarm": int(FP),           
    "missed_fraud": int(FN),          
    "total_transactions": total_rows  
}

# 5. SAVE ARTIFACTS
joblib.dump(model, 'fraud_model_pipeline.pkl')
joblib.dump(X_full.columns.tolist(), 'model_columns.pkl')
joblib.dump(best_threshold, 'model_threshold.pkl') # Save the optimal threshold

# SAVE THE ACCURATE STATS
with open('model_stats.json', 'w') as f:
    json.dump(stats_data, f, indent=4)

print("---")
print("âœ… All files saved. Model trained for low FP and re-optimized threshold.")
print(f"Stats Data Summary:\n{json.dumps(stats_data, indent=4)}")
print("---")
print(f"New False Alarms (FP): {FP}")
print(f"New Missed Fraud (FN): {FN}")