import pandas as pd
import numpy as np
import joblib
import lightgbm as lgb
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

# --- 1. SETUP & CONFIGURATION ---
csv_path = 'creditcard.csv' 
if not os.path.exists(csv_path):
    print("FATAL: creditcard.csv not found. Please ensure it's in the current folder.")
    exit()

# CORRECTED COLUMN NAMES (MATCHING YOUR INPUT LIST)
CUSTOMER_ID_COL = 'Customer ID' 
MERCHANT_ID_COL = 'Merchant Name' 
AMOUNT_COL = 'Transaction Amount' 
TIME_COL = 'trans_date_trans_time' 

print("Loading and preprocessing data...")
df = pd.read_csv(csv_path) 

# Convert the combined date/time column to a proper datetime object
try:
    # Ensure this works with your data format. dayfirst=True is often needed for European formats.
    df['TX_DATETIME'] = pd.to_datetime(df[TIME_COL], dayfirst=True) 
except Exception as e:
    print(f"FATAL: Failed to convert column '{TIME_COL}' to datetime. Check format. Error: {e}")
    exit()

# IMPORTANT FIX: Sort by time and reset index
df = df.sort_values('TX_DATETIME').reset_index(drop=True) 

# 2. PREPARE DATA & FEATURE ENGINEERING
target_col = 'is_fraud' 
y_full = df[target_col]

# --- CRITICAL FIX: Isolate columns for reliable rolling window calculation ---
# Create a copy with only the necessary columns + the new datetime column
df_work = df[[CUSTOMER_ID_COL, MERCHANT_ID_COL, AMOUNT_COL, 'TX_DATETIME']].copy()

# ====================================================================
# A D V A N C E D   F E A T U R E   E N G I N E E R I N G
# ====================================================================

print("Creating advanced behavioral features...")

# Define the features to calculate
features_to_calc = [
    # (new_col, group_col, window, agg_col, agg_func)
    ('C_T_Freq_7D', CUSTOMER_ID_COL, '7D', AMOUNT_COL, 'count'),
    ('C_Avg_Amt_7D', CUSTOMER_ID_COL, '7D', AMOUNT_COL, 'mean'),
    ('M_T_Freq_30D', MERCHANT_ID_COL, '30D', AMOUNT_COL, 'count')
]
engineered_features_list = []

# Loop through and calculate each rolling feature using a merge-back strategy
for new_col, group_col, window, agg_col, agg_func in features_to_calc:
    # 1. Perform the rolling calculation (Result is a MultiIndex Series)
    # The rolling window includes the current transaction's group (ID) and time (TX_DATETIME)
    rolling_series = df_work.groupby(group_col).rolling(window, on='TX_DATETIME')[agg_col].agg(agg_func)
    
    # 2. Convert the MultiIndex result into a DataFrame with columns matching the MultiIndex levels
    rolling_result_df = rolling_series.reset_index()
    
    # 3. Rename the aggregated value column
    rolling_result_df = rolling_result_df.rename(columns={agg_col: new_col})
    
    # 4. Merge back to the original df_work using the unique keys (ID + TX_DATETIME)
    # The merge should happen on the original index of the transaction to align correctly
    df_work = df_work.merge(rolling_result_df, on=[group_col, 'TX_DATETIME'], how='left')
    engineered_features_list.append(new_col)

# Extract the new features
engineered_features = df_work[engineered_features_list]

# Start X_full from all numerical features of the original df (including V1-V28, Time, Amount, is_fraud)
X_full = df.select_dtypes(include=[np.number]).copy()
X_full = X_full.set_index(df.index) # Ensure X_full has the clean index from df

# Merge the new engineered features using the index (guarantees alignment)
# engineered_features is also aligned to df's index after the merge-back loop
X_full = pd.concat([X_full, engineered_features], axis=1)

# Drop redundant/unnecessary numerical columns before scaling
# Drop the target column and the original Time/Amount columns
X_full = X_full.drop(columns=[target_col, 'Amount', 'Time'], errors='ignore') 
X_full = X_full.select_dtypes(include=[np.number]) 

# ====================================================================
# F E A T U R E   T R A N S F O R M A T I O N
# ====================================================================

print("Applying feature transformations (Log & Scaling)...")

# 1. Log Transform Skewed Amount Feature
X_full[AMOUNT_COL + '_log'] = np.log(X_full[AMOUNT_COL] + 0.001) 
X_full = X_full.drop(columns=[AMOUNT_COL], errors='ignore') 

# 2. Standard Scale ALL Features
scaler = StandardScaler()
X_full_cols = X_full.columns
# CRITICAL FIX: Preserve the index after scaling
X_full = pd.DataFrame(scaler.fit_transform(X_full), columns=X_full_cols, index=X_full.index)

# Final cleanup (replace NaNs generated by rolling/scaling, e.g., the first 7 days)
X_full = X_full.fillna(0) 

# --- CRITICAL FINAL ALIGNMENT STEP TO FIX THE ERROR ---
# Ensure X_full and y_full have the exact same indices and number of rows.
common_index = X_full.index.intersection(y_full.index)
X_full = X_full.loc[common_index]
y_full = y_full.loc[common_index] 
# This alignment step guarantees both X_full and y_full have the same number of rows.

total_rows = len(X_full)
print(f"Total rows used for training: {total_rows}")
print(f"Final X_full shape: {X_full.shape}")
print(f"Final y_full shape: {y_full.shape}")

# 3. TRAIN & TEST MODEL 
# This should now work without the ValueError
X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)

# --- CALCULATE CLASS WEIGHTS ---
positive_class_count = y_train.sum()
negative_class_count = len(y_train) - positive_class_count
scale_weight_raw = negative_class_count / positive_class_count

# CRITICAL FIX: CAP THE SCALE WEIGHT TO 50 
MAX_SCALE_WEIGHT = 50 
scale_weight_capped = min(scale_weight_raw, MAX_SCALE_WEIGHT)

print(f"Using capped weight: {scale_weight_capped:.2f}")

# Re-initialize the model with the capped class weights
model = lgb.LGBMClassifier(
    random_state=42,
    scale_pos_weight=scale_weight_capped, 
    n_estimators=100,
    learning_rate=0.05
)

model.fit(X_train, y_train)
print("Model training complete with advanced features and transformations.")

# --- 4. GENERATE STATS ON THE FULL DATASET ---
y_pred_full = model.predict(X_full)
cm_full = confusion_matrix(y_full, y_pred_full)

# Confusion Matrix: [[TN, FP], [FN, TP]]
TN, FP, FN, TP = cm_full.ravel()

# Calculate statistics needed for your dashboard charts
stats_data = {
    "actual_fraud": int(y_full.sum()),
    "actual_legit": int(total_rows - y_full.sum()),
    "pred_fraud": int(TP + FP),       
    "pred_legit": int(TN + FN),       
    "correct_count": int(TN + TP),   
    "false_alarm": int(FP),           
    "missed_fraud": int(FN),          
    "total_transactions": total_rows  
}

# 5. SAVE ARTIFACTS
joblib.dump(model, 'fraud_model_pipeline.pkl')
joblib.dump(X_full.columns.tolist(), 'model_columns.pkl')

# SAVE THE ACCURATE STATS
with open('model_stats.json', 'w') as f:
    json.dump(stats_data, f, indent=4)

print("\nâœ… All files saved. Model re-trained with feature scaling.")
print(f"Stats Data Summary:\n{json.dumps(stats_data, indent=4)}")